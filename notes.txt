## Apropos:
  Content is versioned in git.  commits, issues and pull-requests welcome!
@[https://www.github.com/earizon/PostgreSQL]

[[{01_PM.resource]]
## External Links  
* Manuals: (High quality)               @[https://www.postgresql.org/docs/manuals/]
* PostgreSQL internals (Hironobu SUZUKI) [TODO]
@[http://www.interdb.jp/pg/index.html]
* DB engines Rank:
@[https://db-engines.com/en/ranking]
* Percona Blog:
@[https://www.percona.com/]
* PSQL Hackers list (Patches, dev. discussions, ...)
@[https://www.postgresql.org/list/pgsql-hackers/]

## Bibliography
* P. A. Alsberg and J. D. Day. A principle for resilient
  sharing of distributed resources. In Proceedings of the 2Nd International Conference on Software Engineering
  ICSE ’76, pages 562–570, Los Alamitos, CA, USA, 1976. IEEE Computer Society Press.
*  E. Cecchet, G. Candea, and A. Ailamaki.
  Middleware-based database replication: The gaps between theory and practice. In Proceedings of the
  2008 ACM SIGMOD International Conference on Management of Data , SIGMOD ’08, pages 739–752,
  New York, NY, USA, 2008. ACM
* M. Stonebraker. Concurrency control and consistency of multiple copies of data in distributed ingres.
  IEEE Transactions on Software Engineering,
  SE-5(3):188–194, May 1979
* M. Wiesmann, F. Pedone, A. Schiper, B. Kemme, and G. Alonso. Understanding replication in databases
  and distributed systems. In Proceedings 20th IEEE International Conference on Distributed Computing
  Systems, pages 464–474, April 2000.
* . H. Thomas. A majority consensus approach to concurrency control for multiple copy databases.
  ACM Trans. Database Syst.  , 4(2):180–209, June 1979. H. Thomas. A majority consensus approach to
  concurrency control for multiple copy databases.  ACM Trans. Database Syst.  , 4(2):180–209, June 1979
* M. Wiesmann, F. Pedone, A. Schiper, B. Kemme, and G. Alonso. Understanding replication in databases
  and distributed systems. In Proceedings 20th IEEE International Conference on Distributed Computing
  Systems , pages 464–474, April 2000.
[[}]]

# PostgreSQL 101 [[{devops.101,security.disaster_recovery,security.aaa]]

* ```
  | Core ENV.VARS*               *Show DB info*
  | PGPORT                        mydb=> SELECT version();
  | PGUSER  (alt. -U option)      mydb=> SELECT current_date;
  | PGPASSWORD                    mydb=> SELECT current_database();
  | PGHOST ?                      mydb=> SELECT rolname  FROM pg_roles; ← List users
  |                               - A ROLE is an entity that can own database objects and have database
  |                                 privileges. It can be considered a "user", a "group", or both depending on
  |                                 ussage.
  |                               - New roles can be created with CREATE ROLE
  ```
* psql commands:
  ```
  | \?
  | \h
  | \l            list databases
  | \c db_name    connect to ddbb
  | \q            quit
  | \dt           show all tables in ddbb
  | \dt *.*.      show all tables globally
  | \d  table     show table schema
  | \d+ table
  | \du           list current user's permissions
  | \u
  ```
* PostgresSQL instance entities "layout":                               

  ```
  [[{02_doc_has.diagram}]]
  |                Catalog:(also known as Databases)
  |                contains all the schemas, records, (low-level) logs+write-ahead
  |                logs and constraints for tables.
  |                Catalogs are rigidly separated. A client connection can NOT access two
  |                at the same time. Each Catalog has a different set of physical storage,
  |                passwords and permissions.
  |                                  ┌──┴─────┐
  | Server  == DDBB Cluster  1 ←→ N  Catalog    1 ←→ N Schema
  | └──────────┬──────────┘          (Database)        └──┬─┘
  |            │                                 NAMESPACE FOR TABLES
  |            │                                 and security boundary. N schemas can be accessed by
  |            │                                 a given client connection, by prefixing objects with
  |            │                                 the schema name. eg: myschema01.tableXX
  |            │                                 the schema name. eg: myschema02.tableXX
  |  ┌─────────┘                                 └─────────┬─────────┘
  |  ├ *Databases*                    ┌────────────────────┘
  |  │  │                     ┌───────┴───────┐
  |  │  ├─ postgres           *SCHEMA COMMANDS*
  |  │  │  │
  |  │  │  ├─ Casts           \dn  ← list existing schemas
  |  │  │  │                  SELECT schema_name FROM information_schema.schemata;
  |  │  │  ├─ Catalogs ¹      SELECT nspname     FROM pg_catalog.pg_namespace;
  |  │  │  │
  |  │  │  ├─ Event Triggers  @[http://www.postgresql.org/docs/current/static/sql-createschema.html]
  |  │  │  │                 *CREATE SCHEMA*IF NOT EXISTS accountancy;
  |  │  │  ├─ Extensions
  |  │  │  │                  CREATE TABLE accountancy.employee (....); ← employees table scoped
  |  │  │  ├─ Foreing
  |  │  │  │  Data Wrap       @[http://www.postgresql.org/docs/current/static/sql-dropschema.html]
  |  │  │  │                 *DROP SCHEMA*IF EXISTS accountancy CASCADE;
  |  │  │  ├─ Languages      └───┬────────────...
  |  │  │  │                     │
  |  │  │  └─ Schemas  ←─────────┘
  |  │  │     └─ schema01
  |  │  │        └─ table01, table02, ...
  |  │  │
  |  │  ├─ myDDBB01
  |  │  ...
  |  │
  |  ├─ Login/Group @[https://www.postgresql.org/docs/10/static/user-manag.html]
  |  │
  |  └─ Tablespaces   ←  Allow storage-admins define mapping "objects ←→ file system"
  |
  | ¹: Catalog: regular tables used internally by cluster instances to store
  |    schema metadata (tables&columns info, internal bookkeeping, ...)
  |    They are special in the sense that they must not be changed by hand, but
  |    just let the cluster instance manage them.
  ```

* supported isolation levels [[{01_PM.TODO.101]]
  ```
  - READ COMMITTED
  - REPEATABLE READ (i.e., SI)
  - SERIALIZABLE (i.e., SI with detection and mitigation of anomalies)
  ```
    A snapshot comprises of a set of transaction IDs, which were committed as of
  the start of this transaction, whose effects are visible. Each row has two
  additional elements in the header, namely xmin and xmax , which are the IDs
  of the transactions that created and deleted the row, respectively. Note,
  every update to a row is a delete followed by an insert (both in the table
  and index). Deleted rows are flagged by setting xmax instead of being
  actually deleted. In other words, PostgreSQL maintains all versions of a row
  unlike other implementations such as Oracle that update rows in-place and
  keep a rollback log. This is ideal for our goal of building a blockchain that
  maintains all versions of data. For each row, a snapshot checks xmin and xmax
  to see which of these transactions’ ID are included in the snapshot to
  determine row visibility
[[}]]


## Bootstrap new Postgresql Server (Cluster)

* Bootstrap Cluster "==" Init storage area (data directory in FS terms)
  ```
  | Server 1 → N Databases
  |
  |  $ sudo su postgres                        ← Switch to postgres OS user
  |  $ initdb -D /usr/local/pgsql/data         ← alt.1
  |  $ pg_ctl -D /usr/local/pgsql/data initd   ← alt.2
  |    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  |   'postgres' and 'template1' ddbbs will be created automatically
  ```

## Starting the Server
  ```
  | $ sudo su postgres                       ← Switch to postgres OS user
  | $ postgres -D /usr/local/pgsql/data \    ← Alt.1
  | $ 1>/var/log/postgresql/logfile 2>&1 &
  | $ pg_ctrl start \                        ← Alt.2. Using pg_ctl "easy" wrapper
  | $   -l /var/log/postgresql/logfile'
  | 
  | $ sudo su root                           ← Alt.3. SystemD (recomended in SystemD enabled OSs)
  | $ systemctl enable postgresql.service    ← Enable at OS reboot
  | $ systemctl start postgresql.service     ← Start now without rebooting
  | $ journalctl --unit postgresql.service   ← Check SystemD unit logs
  ```

## CRUD Users
  ```
  | $ sudo su postgres                       ← Switch to postgres OS user
  | $ psql
  |  CREATE USER IF NOT EXISTS myUser01 WITH PASSWORD 'my_user_password';
  |  ALTER USER myUser01 WITH PASSWORD 'my_new_password';
  |  DROP USER IF EXISTS myUser01 ;
  ```

## Create/Remove new DDBB:
  (PRE-SETUP: Create user account for the new DDBB, ex: department01, ...)
  ```
  | $ createdb mydb  ← create new
  | $ psql mydb      ← access it to check everything is OK
  | $ dropdb   mydb  ← Remove. R WARN : can NOT be undone
  |                    NOTE:  Many tools assume ddbb names -U flag or PGUSER
  |                           Env.Var as username by default
  ```

## Granting privileges to users

  ```
  | # GRANT ALL PRIVILEGES ON tableXXX        TO myUser01;
  | # GRANT ALL PRIVILEGES ON DATABASE myDB01 TO myUser01; ← (tables, connections, ...)
  | # GRANT CONNECT        ON DATABASE myDB01 TO myUser01;
  | # GRANT USAGE          ON SCHEMA public   TO myUser01;
  | # GRANT EXECUTE        ON ALL FUNCTIONS
  |                          IN SCHEMA public TO myUser01;
  | # GRANT SELECT,UPDATE  ON ALL TABLES
  |         INSERT,DELETE,   IN SCHEMA public TO myUser01;
  |         CREATE,??
  ```

## CREATE TABLE

  ```
  | -- DO $$
  | -- BEGIN
  | --   EXECUTE 'ALTER DATABASE ' || current_database() || ' SET TIMEZONE TO UTC';
  | -- END; $$;
  | CREATE TABLE IF NOT EXISTS myTable01 (
  |   ID         VARCHAR(40)             NOT NULL           ← or ID serial PRIMARY KEY,
  |                                      CONSTRAINT
  |                                      MYTABLE01_PK PRIMARY KEY ,
  |   NAME       VARCHAR(255)            NOT NULL DEFAULT '#UNKNOWN#',
  |   CREATED_AT TIMESTAMP DEFAULT NOW() NOT NULL,
  |   PUB_KEY    VARCHAR(88)             NOT NULL UNIQUE,
  |   PARENT_ID  VARCHAR(40)           R NULL
  |                                      CONSTRAINT
  |                                         myTable01_FK REFERENCES myTable01(ID),
  |   QUANTITY   INTEGER                 NOT NULL CHECK (QUANTITY >= 1) DEFAULT 1,
  |   RATIO      NUMERIC(5,2)            NOT NULL CHECK (RATIO >= 1.5 ) DEFAULT 2,
  |   ACTIVE     BOOLEAN                 NOT NULL
  |   CREATED_AT TIMESTAMPTZ             DEFAULT now()
  | );
  | 
  | -- CREATE NEW INDEX
  | CREATE IF NOT EXISTS INDEX  MYTABLE01_PUBKEY_IDX ON  myTable01 (PUB_KEY);
  | 
  | -- CREATE COMMENTS
  | COMMENT ON TABLE  myTable01      IS '....';
  | COMMENT ON COLUMN myTable01.ID   IS '....';
  | COMMENT ON COLUMN myTable01.NAME IS '....';
  |
  | ALTER TABLE myTable01 ADD PRIMARY KEY (ID); ← Alternative to create PRIMARY KEY
  ```

# ROLES  @[https://www.postgresql.org/docs/9.0/static/sql-alterrole.html]
  ```
  | ALTER ROLE name [ [ WITH ] option [ ... ] ]
  | 
  | where option can be:
  |       SUPERUSER  | NOSUPERUSER
  |     | CREATEDB   | NOCREATEDB
  |     | CREATEROLE | NOCREATEROLE
  |     | CREATEUSER | NOCREATEUSER
  |     | INHERIT    | NOINHERIT
  |     | LOGIN      | NOLOGIN
  |     | CONNECTION LIMIT connlimit
  |     | [ ENCRYPTED | UNENCRYPTED ] PASSWORD 'password'
  |     | VALID UNTIL 'timestamp'
  |   ALTER ROLE name RENAME TO new_name
  | 
  |   ALTER ROLE name [ IN DATABASE database_name ] SET configuration_parameter { TO | = } { value | DEFAULT }
  |   ALTER ROLE name [ IN DATABASE database_name ] SET configuration_parameter FROM CURRENT
  |   ALTER ROLE name [ IN DATABASE database_name ] RESET configuration_parameter
  |   ALTER ROLE name [ IN DATABASE database_name ] RESET ALL
  ```

## BACKUP/RESTORE    [disaster_recovery]

  ```
  | BACKUP                              RESTORE
  |$ pg_dump ${dbName} > dbName.sql     $ pg_restore -d ddbb_name  -a backup.sql
  |  └──────┬────────┘                    └──────┬────────┘
  |  backup full ddbb (schema + data).    restore full ddbb (schema + data).
  |  (or --data-only | --schema-only )    (or --data-only | --schema-only )
  |$ pg_dumpall > pgbackup.sql
  |  └───┬─────┘
  |  backup all ddbbs
  ```

## EXPORT/IMPORT (COPY) file
  ```
  | \copy myTable01            TO   '/home/user/weather.csv' CSV
  | \copy myTable01(col1,col2) TO   '/home/user/weather.csv' CSV
  |  
  | \copy myTable01            FROM '/home/user/weather.csv' CSV
  | \copy myTable01(col1,col2) FROM '/home/user/weather.csv' CSV
  ```

## Table Maintenance
  ```
  | VACUUM ANALYZE table;     ← VACUUM: (Compact table after many deletion holes)
  | 
  | REINDEX DATABASE dbName;  ← Reindex a database, table or index [performance]
  ```

## Rotate logs [[{01_PM.TODO}]]

## AAA "Filters" [[{security.aaa,security.101,devops.101]]

* REF: <https://www.percona.com/blog/2018/09/21/securing-postgresql-as-an-enterprise-grade-environment/>

* Note: "Filter" is an invented but intuitive nomenclature based on IP filters.

* **WARN**: initial AAA setup allows any local user connect and become ddbb superuser.
  1. use one `initdb` `-W`|`--pwprompt`|`--pwfile` options
      to assign a password to the database superuser.
  2. `set -A md5 | -A password` to disable default trust Authen. mode.
  3. modify auto-generated *pg_hba.conf* after running initdb and
      before starting server for the first time.

### 1st Filter: host based (pg_hba.conf) to authorize incoming connections

* official doc: <https://www.postgresql.org/docs/devel/static/auth-pg-hba-conf.html>

* IMPORTANT:  order of the entries matters.
  ```
  |  host    · ddbb1 · pguser · ip1/32 · md5 R *1            ← connections from 'ip1' only allowed
  |          ·       ·        ·        ·                      from user pguser and only to ddbb percona.
  |          ·       ·        ·        ·                      using  md5 password authentication.  *2
  | 
  |  hostssl · all   · all    · ip2/32 · md5               ← SSL connections allowed
  |          ·       ·        ·        ·                      to any user@ip2 to any ddbb
  | 
  |  hostssl · all   · all    · ip3/32 · cert clientcert=1 ← SSL connections allowed
  |          ·       ·        ·        ·                      to any user@192.68.0.13 to any ddbb
  | └──┬───┘  └─┬──┘   └─┬──┘   └─┬──┘   └──────┬────────┘    presenting valid client cert.  *2
  |   TYPE     DDBB     USER   IP RANGE   AUTHEN.METHOD
  |                                       (MD5, SCRAM,SSL certs,
  |                                        PAM/LDAP, Kerberos,...) *3
  | 
  |  *1  MD5 hasing problem: always return same hash for same password.
  |      - in ver.10+ prefer SCRAM (SHA-256 with salts)
  |        for extra security against password-dictionary attacks.
  |  *2  @[https://www.postgresql.org/docs/10/static/ssl-tcp.html]
  |  *3  @[https://www.postgresql.org/docs/10/static/client-authentication.html]
  ```

### 2nd Filter: roles and privileges 
  ```
  | user 1 ←-·······→ N Roles 1←···········→ M privileges
  |
  | developer01 ←─┐*1 dev_read_only              ...
  | developer02 ←─┴─→ dev_read_write
  |                   app_read_only
  |                   app_read_write
  |                   admin_read_only        ┌→ SELECT on hhrr.employee
  |                   admin_read_write       ├→ INSERT on hhrr.employee
  | manager01 ←─*2 ─→ managers ←─┐           ├→ UPDATE on hhrr.employee
  | manager02 ←─*2 ─→ managers ←─┴───────────┴→ DELETE on hhrr.employee
  | ^                                  ^                  ^
  | ·     *1 GRANT dev_read_wite to \  ·     CREATE TABLE hhrr.employee (
  | ·        developer01, developer02; ·       id INT,
  | ·                                  ·       first_name VARCHAR(20), ...,
  | ·     *2 CREATE ROLE managers;     ·       manager VARCHAR(20) );
  | ·     *2 GRANT managers      to \  ·
  | ·        manager01, manager02;     ·
  | ·        ┌·························┘
  | ·        GRANT SELECT, INSERT, UPDATE, DELETE \
  | ·        ON hhrr.employee TO managers;
  | ·
  | └······· CREATE USER manager01 WITH ENCRYPTED PASSWORD 'manager01';
  |          CREATE USER manager02 WITH ENCRYPTED PASSWORD 'manager02';
  ```

### 3rd Filter, Row level Security (9.5+)
  ```
  |  INSERT INTO hhrr.employee VALUES \
  |    (1, 'user01','...','manager01'),
  |    (2, 'user02','...','manager02'),
  |    (3, 'user03','...','manager01');
  |
  |  GRANT USAGE ON SCHEMA hhrr TO managers;
  |  ALTER TABLE hhrr.employee ENABLE ROW LEVEL SECURITY;
  |
  |  CREATE POLICY employee_managers  ← allows the managers role to
  |    ON hhrr.employee                 only view/modify their own
  |    TO managers                      subordinates’ records:
  |    USING (manager = current_user);
  |
  |  $ psql -d ddbb01 -U manager01              $ psql -d ddbb01 -U manager02
  |  ...                                        ...
  |  =→  select * from hhrr.employee ;          =→  select * from hhrr.employee ;
  |  id │ first_name │ last_name │ manager      id │ first_name │ last_name │ manager
  |  ───┼────────────┼───────────┼─────────    ────┼────────────┼───────────┼─────────
  |   1 │ user01     │ ...       │ manager01     1 │ user02     │ ...       │ manager02
  |   3 │ user03     │ ...       │ manager01
  |   (2 rows)                                   (1 rows)
  ```

### 4th Filter. DATA AT RESET ENCRYPTION OPTIONS  [[{01_PM.TODO]]
* <https://www.postgresql.org/docs/10/static/encryption-options.html>
* The pgcrypto module allows certain fields to be stored encrypted. This
  is useful if only some of the data is sensitive. The client supplies the
  decryption key and the data is decrypted on the server and then sent to the client. [[}]]
[[}]]

## pg_file_settings                                                      [[{01_PM.TODO.101]]
* <https://www.postgresql.org/docs/10/static/view-pg-file-settings.html>
* <https://www.postgresql.org/docs/10/static/config-setting.html>
* Server instance configuration
* can be used to debug or pre-test changes in configuration
[[}]]

## postgresql.conf (Resource tunning) [[{devops.101,01_PM.low_code,performance.troubleshooting]]
@[https://www.postgresql.org/docs/10/static/config-setting.html]

* Set config Settings for logs, buffers, cahce, ...
  * Re-read changes with *SIGHUP*signal or $*pg_ctl reload*

###  PGConfig: Online postgresql.conf tool: [[{PM.low_code,performance]]
* <https://www.pgconfig.org/>

 ```
 | Example INPUT:            →  OUTPUT
 |
 | DB Version: 13               max_connections = 20
 | OS Type: linux               shared_buffers = 3840MB
 | DB Type: web                 effective_cache_size = 11520MB
 | Total Memory (RAM): 15 GB    maintenance_work_mem = 960MB
 | CPUs num: 4                  checkpoint_completion_target = 0.7
 | Connections num: 20          wal_buffers = 16MB
 | Data Storage: ssd            default_statistics_target = 100
 |                              random_page_cost = 1.1
 |                              effective_io_concurrency = 200
 |                              work_mem = 96MB
 |                              min_wal_size = 1GB
 |                              max_wal_size = 4GB
 |                              max_worker_processes = 4
 |                              max_parallel_workers_per_gather = 2
 |                              max_parallel_workers = 4
 |                              max_parallel_maintenance_workers = 2
 ```
[[}]]


### PgTune Config.Wizard [[{PM.low_code,performance]]
* <https://github.com/gregs1104/pgtune>
* Python script taking the wimpy default postgresql.conf and
expanding the cluster server to be as powerful as the hardware it's
being deployed on.
[[}]]

## Logging in PostgreSQL   [[{monitor]]
* log either:
  * all of the statements
  * a few statements based on parameter settings.

* You can log all the DDLs or DMLs or any statement running for more
  than a certain duration to the log file when logging_collector is
  enabled. To avoid write overload to the data directory, you may also
  move your log_directory to a different location. Here’s a few
  important parameters you should review when logging activities in
  your PostgreSQL server:
  * log_connections
  * log_disconnections
  * log_lock_waits
  * log_statement
  * log_min_duration_statement
* Please note that detailed logging takes additional disk space and may
  impose an important overhead in terms of write IO depending on the
  activity in your PostgreSQL server. You should be careful when
  enabling logging and should only do so after understanding the
  overhead and performance degradation it may cause to your workload.
[[}]]

# PostgreSQL performance tunning, general guidelines  [[{performance.101]]
* Enable autovacuum. The working memory for autovacuum should be no more than 2% of
  the total available memory.
* Enable database caching with an effective cache size between 6% and 8% of the total
  available memory.
* To increase performance, the working memory should be at least 10% of the total
  available
  Set SERVER_ENCODING , LC_COLLATE and LC_CTYPE as :
  ```
  | server_encoding = UTF8
  | lc_collate = en_US.UTF-8
  | lc_ctype = en_US.UTF-8
  ```
* (Shared Memory/Semaphores/...):
* Show all runtime parameters:
  ```
  #- SHOW ALL;*
  ```
[[}]]

# Linux performance tunning, general guidelines:  [[{performance,storage.RAID,]]

* <https://paquier.xyz/postgresql-2/tuning-disks-and-linux-for-postgres.markdown/>

* Choose correct RAID strategy:
  * RAID0 :  more disks, more performance, more risk of data loss.
  * RAID1 :  good for medium workloads redundancy.  Good performance for read applications
  * RAID5 :  should be used for large workloads with more than 3 disks.
* File systems:
  * ext4: fast and stable.
  * xfs : good performance for many parallel processes.
  * FS "noatime" must be disabled: The database server does not use the info about file update times.
  * put data-folder on a separate disk.
  * Separate certain tables highly read or written on dedicated disks.
  * Try to put 'pg_xlog' on isolated disk.
* I/O scheduler (/sys/block/$DISK_NAME/queue/scheduler)
  * deadline: can offer good response times with mix of read/write loads and maintains a good throughput.
              Choose this wisely depending on your hardware though!
  * noop    :
  * "anything wanted":
  * Linux readahead:
    Allowing to put a file's content into page cache instead of disk, making reading this file faster when
    it is subsequently accessed.
    * Can be set with: `$ blockdev -setra (-getra to read it)`
    * On most of distributions it is set on a low value, 256~1024 512blocks /(128kB~512kB).
    * A higher value can really improve sequential scans for large tables,
      as well as index scans on a large number of rows.
      * Start up with 4096 and see the results! Besides.
      * Be aware that values higher than 16MB (32768 for 512 byte sectors) do not show
        that much performance gain (thanks Dimitri Fontaine for this info!).
  * Transparent Huge Pages (THP) problems
    * REF: <https://docs.mongodb.com/manual/tutorial/transparent-huge-pages/>
    * Transparent Huge Pages (THP) is a Linux memory management system that reduces
      the overhead of Translation Lookaside Buffer (TLB) lookups on machines with
      large amounts of memory by using larger memory pages.
      However, database workloads often perform poorly with THP enabled, because they
      tend to have sparse rather than contiguous memory access patterns. When running
      MongoDB on Linux, THP should be disabled for best performance.
    * TODO: Does the same problem applies to PSQL?

* See also: <https://postgreshelp.com/postgresql-kernel-parameters/>  [[{01_PM.TODO}]]
[[}]]

# Network latency does BIG difference [[{performance.network,performance.101]]
* <https://www.cybertec-postgresql.com/en/postgresql-network-latency-does-make-a-big-difference/>
(Measuring the performance impact caused by latency)

* "tc": linux command to control network settings in Linux kernel.
  It allows you to do all kinds of trickery such as adding latency,
  bandwidth limitations and so on.

* tcconfig: python wrapper around tc.
  ```$ sudo pip3 install tcconfig```

* compare performance vs network latency:
  ```
  | $ createdb test
  | $ pgbench -i test
  | dropping old tables...
  | ...
  | generating data...
  | 100000 of 100000 tuples (100%) done (elapsed 0.11 s, remaining 0.00 s)
  | vacuuming...
  | creating primary keys...
  | 
  |  NO NETWORK LATENCY                  10 MS NETWORK LATENCY
  |                                      (# tcset --device lo --delay=10)
  │  ─────────────────────────────────   ────────────────────────────────
  |  $ pgbench -S -c 10 -h localhost \   $ pgbench -S -c 10 -h localhost \
  |    -T 20 test                          -T 20 test
  |  starting vacuum...end.              starting vacuum...end.
  |  transaction type:                   transaction type:
  |  scaling factor: 1                   scaling factor: 1
  |  query mode: simple                  query mode: simple
  |  number of clients: 10               number of clients: 10
  |  number of threads: 1                number of threads: 1
  |  duration: 20 s                      duration: 20 s
  |  number of TX processed: 176300      number of TX processed: 9239
  |  latency average = 1.135 ms          latency average = 21.660 ms
  | *tps = 8813.741733*...              *tps = 461.687712*(...)
  |                                            ^^^
  |                                      20 times slower!!!.
  |                                      Specailly painful on OLTP application
  |                                      (sort queries)
  |                                      With 50ms performance drops 100xtimes!!
  ```
[[}]]

# BLOB cleanup [[{storage.blob]]

* REF: <https://www.cybertec-postgresql.com/en/blob-cleanup-in-postgresql/>
  by Hans-Jürgen Schönig

* bytea : simplest form to use binary data. up to  *1 GB per field*!!.
  The binary field is seen as part of a row. Ex:
  ```
  | # CREATE TABLE t_image (id int, name text,*image bytea*);
  | ...
  | # SHOW bytea_output;
  |   bytea_output
  |   ──────────────
  |   hex         <·· encoding. Send the data in hex format.
  |   (1 row)         (vs "escape" == octal string).
  ```
* BLOBs : More versatile. Ex:
  ```
  | # SELECT lo_import('/etc/hosts');  ← /etc/hosts imported (copy -vs link- of data)
  | lo_import
  | ───────────
  | 80343        ← new OID (object ID) of the new entry.
  | (1 row)
  |
  | - To keep trace of new OIDs we can do something like:
  | # CREATE TABLE t_file ( id int, name text, object_id oid);
  |
  | # INSERT INTO t_file VALUES
  |   (1, 'some_name', lo_import('/etc/hosts'))
  |
  | # DELETE FROM t_file WHERE id = 1;
  |   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  | R*PROBLEM*: object id has been "forgotten".
  |   but it is still there.
  |  *'pg_largeobject'*is the*system table*in charge
  |   of storing the binary data inside PostgreSQL.
  |   All lo_* functions "talk" to this system table
  |
  | # \x ← Expanded display is on
  | test=# SELECT * FROM pg_largeobject WHERE loid = 80350;
  | ─[ RECORD 1 ]──────────────────────────────────────────
  | loid   | 80350       ← R*Dead object*
  | pageno | 0
  | data   | ##\012# Host Database\012#\012# localhost ...
  |
  |
  | # SELECT lo_unlink(80350);  *Correct way to clean BLO *
  |
  | # SELECT * FROM pg_largeobject  ← Recheck
  |   WHERE loid = 80350;
  | (0 rows)                        ← OK
  ```

* Automated cleaning of dead large objects:
  ```
  | $ vacuumlo -h localhost -v test         
  | ...                                     
  | Successfully removed 2 large objects ...
  ```

* Other functions provided by PSQL for large objects include:

  ```
  | # \df lo_*
  |                                 List of functions
  |    Schema   |   Name        | Result data type | Argument data types       | Type
  | ────────────┼───────────────┼──────────────────┼───────────────────────────┼──────
  |  pg_catalog | lo_close      | integer          | integer                   | func
  |  pg_catalog | lo_creat      | oid              | integer                   | func
  |  pg_catalog | lo_create     | oid              | oid                       | func
  |  pg_catalog | lo_export     | integer          | oid, text                 | func
  |  pg_catalog | lo_from_bytea | oid              | oid, bytea                | func
  |  pg_catalog | lo_get        | bytea            | oid                       | func
  |  pg_catalog | lo_get        | bytea            | oid, bigint, integer      | func
  |  pg_catalog | lo_import     | oid              | text                      | func
  |  pg_catalog | lo_import     | oid              | text, oid                 | func
  |  pg_catalog | lo_lseek      | integer          | integer, integer, integer | func
  |  pg_catalog | lo_lseek64    | bigint           | integer, bigint, integer  | func
  |  pg_catalog | lo_open       | integer          | oid, integer              | func
  |  pg_catalog | lo_put        | void             | oid, bigint, bytea        | func
  |  pg_catalog | lo_tell       | integer          | integer                   | func
  |  pg_catalog | lo_tell64     | bigint           | integer                   | func
  |  pg_catalog | lo_truncate   | integer          | integer, integer          | func
  |  pg_catalog | lo_truncate64 | integer          | integer, bigint           | func
  |  pg_catalog | lo_unlink     | integer          | oid                       | func
  | (18 rows)
  |
  |  pg_catalog | loread        | bytea            | integer, integer          | func
  |  pg_catalog | lowrite       | integer          | integer, bytea            | func
  ```

- BLOB interface is fully transactional: binary content and metadata cannot go
  out of sync anymore.
[[}]]


# SQL Optimizations: IN vs EXISTS vs ANY/ALL vs JOIN  [[{101,performance.101]]
* <https://www.percona.com/blog/2020/04/16/sql-optimizations-in-postgresql-in-vs-exists-vs-any-all-vs-join/>

 There are multiple ways in which a sub select or lookup can be framed
in a SQL statement. PostgreSQL optimizer is very smart at optimizing
queries, and many of the queries can be rewritten/transformed for
better performance.

In general, I used to suggest to developers that the key to writing a
good SQL statement is to follow step by step process.

* First, make a list of tables from which the data should be retrieved.
* Then think about how to JOIN those tables.
* Think about how to have the minimum records participating in the join condition.
[[}]]

# Execution plans explained [[{101,performance.101]]
* REF: <https://explain.dalibo.com/>

* EXPLAIN ANALYZE is a prefix to a regular query that
  explains why a query will take a certain amount of time, instead of
  actually executing it. Ex:
  ```
  | => EXPLAIN ANALYZE
  |    SELECT * FROM some_view
  |      WHERE nspname not in
  |        ('pg_catalog', 'information_schema') order by 1, 2, 3;
  | 
  |    QUERY PLAN
  | -------------------------------------------------
  |  Sort  (cost=146.63..148.65 rows=808 width=138) (actual time=55.009..55.012 rows=71 loops=1)
  |    Sort Key: n.nspname, p.proname, (pg_get_function_arguments(p.oid))
  |    Sort Method: quicksort  Memory: 43kB
  |    →  Hash Join  (cost=1.14..107.61 rows=808 width=138) (actual time=42.495..54.854 rows=71 loops=1)
  |          Hash Cond: (p.pronamespace = n.oid)
  |          →  Seq Scan on pg_proc p  (cost=0.00..89.30 rows=808 width=78) (actual time=0.052..53.465 rows=2402 loops=1)
  |                Filter: pg_function_is_visible(oid)
  |          →  Hash  (cost=1.09..1.09 rows=4 width=68) (actual time=0.011..0.011 rows=4 loops=1)
  |                Buckets: 1024  Batches: 1  Memory Usage: 1kB
  |                →  Seq Scan on pg_namespace n  (cost=0.00..1.09 rows=4 width=68) (actual time=0.005..0.007 rows=4 loops=1)
  |                      Filter: ((nspname <> 'pg_catalog'::name) AND (nspname <> 'information_schema'::name))
  ```
[[}]]

## Pev2
* https://github.com/dalibo/pev2
* VueJS component to show a graphical vizualization of a PostgreSQL execution plan.
* https://explain.depesz.com/


# Containerization 101

* Crunchy low-code deployments [[{01_PM.low_code,01_PM.TODO.101}]]
  * <https://crunchydata.github.io/crunchy-containers/>
     The Crunchy Container Suite provides Docker containers that enable
  rapid deployment of PostgreSQL, including administration and
  monitoring tools. Multiple styles of deploying PostgreSQL clusters
  are supported.
* DETAILED shell script + dockerized run: [[{devops.containerization.101,devops.shell_script]]

  ```
  | D_RUN_OPTS=""                                     # DOCKER RUN SPECIFIC OPTIONS
  | D_RUN_OPTS="${D_RUN_OPTS} --rm \ "                # <- Remove docker image after execution
  | D_RUN_OPTS="${D_RUN_OPTS} --network host \ "      # <- Optinionated. Use any suitable network
  | D_RUN_OPTS="${D_RUN_OPTS} -i "                    # <- Interactive (Create STDIN for container)
  | D_RUN_OPTS="${D_RUN_OPTS} -e PGUSER=USER01 \"     # <- Inject user (WARN: --username takes precedence)
  | D_RUN_OPTS="${D_RUN_OPTS} -e PGPASSWORD=MYPASS "  # <- Inject password
  | #-----------------------------------------------
  | # psql Non-interactive mode. Exec query and exit
  | # REF: https://dba.stackexchange.com/questions/77917/psql-run-commands-in-batch-mode
  | # REF: https://stackoverflow.com/questions/45352374/psql-non-select-how-to-remove-formatti    ng-and-show-only-certain-columns
  | PSQL_OPTS="${PSQL_OPTS} -q "                   # quiet (recomended for automated scripts)
  | PSQL_OPTS="${PSQL_OPTS} -t "                   # Print only tuples (remove header/footer)
  | PSQL_OPTS="${PSQL_OPTS} -A "                   # un(A)ligned output
  | PSQL_OPTS="${PSQL_OPTS} -X "                   # ignore .psqlrc
  | PSQL_OPTS="${PSQL_OPTS} -v ON_ERROR_STOP=1 "
  | PSQL_OPTS="${PSQL_OPTS} -P pager=off "
  | PSQL_OPTS="${PSQL_OPTS} --single-transaction " # Opinionated.
  | PSQL_OPTS="${PSQL_OPTS} --username=postgres "  # It will overwrite PGUSER injected env.var.
  | PSQL_OPTS="${PSQL_OPTS} --dbname=myDatabase "  #
  | PSQL_OPTS="${PSQL_OPTS} -h localhost "         # host to connect to.
  | PSQL_OPTS="${PSQL_OPTS} -p 5432 "              # Port to connect to.
  | #-----------------------------------------------
  | (                                      <┐
  |   cat << __EOF                          │
  |     SET search_path = schema01;         │  script bash
  |     SELECT TO_TIMESTAMP(secTSRegistry)  │  Any STDOUT(put) from ( ... )
  |     FROM   schema01.table01             ├─ will be injected as STDIN(put)
  |     WHERE TO_TIMESTAMP(secTSRegistry)   │  to psql.
  |           > NOW() - INTERVAL '10 DAY'   │
  | __EOF                                   │
  | ) | \                                  <┘
  |   docker run ${D_RUN_OPTS} \
  |     postgres:14.5 \                    <- Use oficial image with given 14.5 version
  |     psql ${PSQL_OPTS} \
  |     'COPY FROM STDIN'                  <- Take SQL INPUT from STDIN
  ```
[[}]]

# Speed up queries 100x times using UNION [[{101,performance.101,01_PM.TODO.101,01_PM.TODO}]]
* <https://www.foxhound.systems/blog/sql-performance-with-union/>

* One of the most common cases where SQL query performance can degrade
  significantly is in a diamond shaped schema, where there are multiple
  ways of joining two tables together. In such a schema, a query is
  likely to use OR to join tables in more than one way, which
  eliminates the optimizer’s ability to create an efficient query
  plan. This scenario is best illustrated through an example.

  ```
  |                                 stores
  |                                 +---------+------+
  |      customers            +----→| id      | int  |←----------------+
  |      +----------+------+  |     | address | text |                 |
  | +---→| id       | int  |  |     +---------+------+                 |
  | |    | name     | text |  |                                        |
  | |    | store_id | int  +--+                   employees            |
  | |    +----------+------+                      +----------+------+  |
  | |                                      +-----→| id       | int  |  |
  | |                                      |      | name     | text |  |
  | |  customer_orders                     |      | role     | text |  |
  | |  +-------------+-----------+         |      | store_id | int  +--+
  | |  | id          | int       |←--+     |      +----------+------+
  | +--+ customer_id | int       |   |     |
  |    | created     | timestamp |   |     |
  |    +-------------+-----------+   |     |  employee_markouts
  |                                  |     |  +--------------+-----------+
  |                                  |     |  | id           | int       |
  |     customer_order_items         |     +--+ employee_id  | int       |
  |     +-------------------+-----+  |        | meal_item_id | int       +--+
  |     | id                | int |  |        | created      | timestamp |  |
  |     | customer_order_id | int +--+        +--------------+-----------+  |
  |  +--+ meal_item_id      | int |                                         |
  |  |  +-------------------+-----+                                         |
  |  |                                 meal_items                           |
  |  |                                 +-------+------+                     |
  |  +--------------------------------→| id    | int  |←--------------------+
  |                                    | label | text |
  |                                    | price | int  |
  |                                    +-------+------+
  ```

# Understanding PSQL performance
* <https://www.craigkerstiens.com/2012/10/01/understanding-postgres-performance/>

* Generally you want your database to have a cache hit rate of about 99%.
  You can find your cache hit rate with:

  ```
  | SELECT
  |   sum(heap_blks_read) as heap_read,
  |   sum(heap_blks_hit)  as heap_hit,
  |   sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as ratio
  | FROM
  |   pg_statio_user_tables;
  ```

  If you find yourself with a ratio significantly lower than 99% then
  you likely want to consider increasing the cache available to your
  database.
* The other primary piece for improving performance is indexes.
   To generate a list of your tables in your database with the largest
  ones first and the percentage of time which they use an index you can
  run:
  ```
  SELECT
    relname,
    100 * idx_scan / (seq_scan + idx_scan) percent_of_times_index_used,
    n_live_tup rows_in_table
  FROM
    pg_stat_user_tables
  WHERE
      seq_scan + idx_scan > 0
  ORDER BY
    n_live_tup DESC;
  ```
     While there is no perfect answer, if you’re not somewhere around
  99% on any table over 10,000 rows you may want to consider adding an
  index.
* Index Cache Hit Rate
  Finally to combine the two if you’re interested in how many of
  your indexes are within your cache you can run:
  ```
  SELECT
    sum(idx_blks_read) as idx_read,
    sum(idx_blks_hit)  as idx_hit,
    (sum(idx_blks_hit) - sum(idx_blks_read)) / sum(idx_blks_hit) as ratio
  FROM
    pg_statio_user_indexes;
  ```
  Generally, you should also expect this to be in the 99% similar to
  your regular cache hit rate.

[[{02_DOC_HAS.comparative,data.analytics]]
# PSQL vs MSQL comparative for data analytics
@[https://www.pg-versus-ms.com/]
* PSQL: PostgreSQL's CSV support is very good with RFC4180 support
  (which is the closest thing there is to an official CSV standard)
  ```
  COPY TO
  COPY FROM
  ```
* Helpul error message in case of error with fail-fast approach:
  * Abort import on problem
  (vs silently corrupt, misunderstand or alter data)

* ```
  | PostgreSQL                     | MS SQL Server:
  | DROP TABLE IF EXISTS my_table; |
  |                                | IF OBJECT_ID (N'dbo.my_table', N'U') IS NOT NULL
  |                                | DROP TABLE dbo.my_table;
  ```
* PSQL supports DROP SCHEMA CASCADE:
  * This is very, very important for a robust analytics delivery methodology,
    where tear-down-and-rebuild is the underlying principle of repeatable,
    auditable, collaborative analytics work.
  * drop  schema and all the database objects inside it.
* ```
  | PostgreSQL                  |  MS SQL Server
  | CREATE TABLE good_films AS  |  SELECT
  | SELECT                      |    *
  |   *                         |  INTO
  | FROM                        |    good_films
  |   all_films                 |  FROM
  | WHERE                       |    all_films
  |   imdb_rating >= 8;         |  WHERE
  ```
* In PostgreSQL, you can execute as many SQL statements as you like in one
  batch; as long as you've ended each statement with a semicolon, you can
  execute whatever combination of statements you like. For executing
  automated batch processes or repeatable data builds or output tasks, this
  is critically important functionality.
* PostgreSQL supports the RETURNING clause, allowing UPDATE, INSERT and
  DELETE statements to return values from affected rows. This is elegant and
  useful. MS SQL Server has the OUTPUT clause, which requires a separate
  table variable definition to function. This is clunky and inconvenient and
  forces a programmer to create and maintain unnecessary boilerplate code.
* PostgreSQL supports $$ string quoting, like so:
  ```SELECT $$Hello, World$$ AS greeting;```
    This is extremely useful for generating dynamic SQL because (a) it allows
  the user to avoid tedious and unreliable manual quoting and escaping when
  literal strings are nested and (b) since text editors and IDEs tend not to
  recogniise $$ as a string delimiter, syntax highlighting remains functional
  even in dynamic SQL code.
* PostgreSQL lets you use procedural languages simply by submitting code to
  the database engine; you write procedural code in Python or Perl or R or
  JavaScript or any of the other supported languages (see below) right next
  to your SQL, in the same script. This is convenient, quick, maintainable,
  easy to review, easy to reuse and so on.
* "Pure" declarative SQL is good at what it was designed for – relational
  data manipulation and querying. You quickly reach its limits if you try to
  use it for more involved analytical processes, such as complex interest
  calculations, time series analysis and general algorithm design. SQL
  database providers know this, so almost all SQL databases implement some
  kind of procedural language. This allows a database user to write imperative
  * style code for more complex or fiddly tasks.
  * PostgreSQL's procedural language support is exceptional:
    * PL/PGSQL: this is PostgreSQL's native procedural language. It's like Oracle's
        PL/SQL, but more modern and feature-complete.
    * PL/V8: the V8 JavaScript engine from Google Chrome is available in PostgreSQL.
      Even better, PL/V8 supports global (i.e. cross-function call) state,
      allowing the user to selectively cache data in RAM for fast random access.
      Suppose you need to use 100,000 rows of data from table A on each of 1,000,000
      rows of data from table B. In traditional SQL, you either need to join
      these tables (resulting in a 100bn row intermediate table, which will
      kill any but the most immense server) or do something akin to a scalar
      subquery (or, worse, cursor-based nested loops), resulting in crippling
      I/O load if the query planner doesn't read your intentions properly.<br/>
       In PL/V8 you simply cache table A in memory and run a function on each         
      of the rows of table B – in effect giving you RAM-quality access (              
      negligible latency and random access penalty; no non-volatile I/O load)         
      to the 100k-row table. I did this on a real piece of work recently – my         
      PostgreSQL/PLV8 code was about 80 times faster than the MS T-SQL solution       
      and the code was much smaller and more maintainable. Because it took about      
      23 seconds instead of half an hour to run, I was able to run 20 run-test-modify 
      cycles in an hour, resulting in feature-complete, properly tested, bug-free     
      code.<br/>
      (All those run-test-modify cycles were only possible because of DROP SCHEMA CASCADE
       and freedom to execute CREATE FUNCTION statements in the middle of a statement
       batch, as explained above. See how nicely it all fits together?)
    * PL/Python: Fancy running a SVM from scikit-learn or some
      arbitrary-precision arithmetic provided by gmpy2 in the middle of a SQL query?
      No problem!
    * PL/R
    * C: doesn't quite belong in this list because you have to compile it
      separately, but it's worth a mention.  In PostgreSQL it is trivially easy
      to create functions which execute compiled, optimised C (or C++ or assembler)
      in the database backend.
* In PostgreSQL, custom aggregates are convenient and simple to use,
  resulting in fast problem-solving and maintainable code:
  ```
  | CREATE FUNCTION interest_sfunc(state JSON, movement FLOAT, rate FLOAT, dt DATE) RETURNS JSON AS
  | $$
  | state.balance += movement;  //payments into/withdrawals from account
  | if (0 === dt.getUTCDate()) //compound interest on 1st of every month
  | {
  |   state.balance += state.accrual;
  |   state.accrual = 0;
  | }
  | state.accrual += state.balance * rate;
  | return state;
  | $$ LANGUAGE plv8;
  | 
  | CREATE AGGREGATE interest(FLOAT, FLOAT, DATE)
  | (
  |   SFUNC=interest_sfunc,
  |   STYPE=JSON,
  |   INITCOND='{"balance": 0, "accrual": 0}'
  | );
  | {
  |   state.balance += state.accrual;
  | 
  | --assume accounts table has customer ID, date, interest rate and account movement for each day
  | CREATE TABLE cust_balances AS
  | SELECT
  |   cust_id,
  |   (interest(movement, rate, dt ORDER BY dt)->>'balance')::FLOAT AS balance
  | FROM
  |   accounts
  | GROUP BY
  |   cust_id;
  | {
  ```
  Elegant, eh? A custom aggregate is specified in terms of an internal state
  and a way to modify that state when we push new values into the aggregate
  function. In this case we start each customer off with zero balance and no
  interest accrued, and on each day we accrue interest appropriately and
  account for payments and withdrawals. We compound the interest on the 1st
  of every month. Notice that the aggregate accepts an ORDER BY clause (since
  , unlike SUM, MAX and MIN, this aggregate is order-dependent) and
  PostgreSQL provides operators for extracting values from JSON objects. So,
  in 28 lines of code we've created the framework for monthly compounding
  interest on bank accounts and used it to calculate final balances. If
  features are to be added to the methodology (e.g. interest rate
  modifications depending on debit/credit balance, detection of exceptional
  circumstances), it's all right there in the transition function and is
  written in an appropriate language for implementing complex logic. (Tragic
  side-note: I have seen large organisations spend tens of thousands of
  pounds over weeks of work trying to achieve the same thing using poorer tools.)
* Date/Time
  * PostgreSQL: you get DATE, TIME, TIMESTAMP and TIMESTAMP WITH TIME ZONE,
  all of which do exactly what you would expect. They also have fantastic
  range and precision, supporting microsecond resolution from the 5th
  millennium BC to almost 300 millennia in the future. They accept input in a
  wide variety of formats and the last one has full support for time zones
  * They can be converted to and from Unix time, which is very important
    for interoperability with other systems.
  * They also support the INTERVAL type, which is so useful it has its own
    section right after this one.
    ```
    | SELECT to_char('2001-02-03'::DATE, 'FMDay DD Mon YYYY');  ← "Saturday 03 Feb 2001"
    | 
    | SELECT to_timestamp('Saturday 03 Feb 2001', 'FMDay DD Mon YYYY');  ←TS 2001-02-03 00:00:00+00
    | 
    | SELECT NOW() - INTERVAL '1 DAY';
    | 
    | SELECT
    |     'yesterday'::TIMESTAMP,
    |     'tomorrow'::TIMESTAMP,
    |     'allballs'::TIME AS aka_midnight;
    ```
  * PostgreSQL: the INTERVAL type represents a period of time, such as "30
    microseconds" or "50 years". It can also be negative, which may seem
    counterintuitive until you remember that the word "ago" exists. PostgreSQL
    also knows about "ago", in fact, and will accept strings like '1 day ago'
    as interval values (this will be internally represented as an interval of -
    1 days). Interval values let you do intuitive date arithmetic and store
    time durations as first-class data values. They work exactly as you expect
    and can be freely casted and converted to and from anything which makes sense
* PostgreSQL arrays are supported as a first-class data type
  * eaning fields in tables, variables in PL/PGSQL, parameters to functions
    and so on can be arrays. Arrays can contain any data type you like,
    including other arrays. This is very, very useful. Here are some of the
    things you can do with arrays:
  * Store the results of function calls with arbitrarily-many return values, such as regex matches
  * Represent a string as integer word IDs, for use in fast text matching algorithms
  * Aggregation of multiple data values across groups, for efficient cross-tabulation
  * Perform row operations using multiple data values without the expense of a join
  * Accurately and semantically represent array data from other applications in your tool stack
  * Feed array data to other applications in your tool stack
* PostgreSQL: full support for JSON, including a large set of utility functions for
   transforming between JSON types and tables (in both directions)
* PostgreSQL: HSTORE is a PostgreSQL extension which implements a fast key-value store as a data type.
   Like arrays, this is very useful because virtually every high-level programming language has such
   a concept (associative arrays, dicts, std::map ...)
   There are also some fun unexpected uses of such a data type. A colleague
   recently asked me if there was a good way to deduplicate a text array. Here's
   ```
   |  SELECT akeys(hstore(my_array, my_array)) FROM my_table;
   ```
   i.e. put the array into both the keys and values of an HSTORE, forcing a
   dedupe to take place (since key values are unique) then retrieve the keys
   from the HSTORE. There's that PostgreSQL versatility again.
* PostgreSQL: [range types](https://www.postgresql.org/docs/9.3/static/rangetypes.html)
  Every database programmer has seen fields called start_date and end_date,
  and most of them have had to implement logic to detect overlaps. Some have even found, the hard way,
  that joins to ranges using BETWEEN can go horribly wrong, for a number of reasons.
  PostgreSQL's approach is to treat time ranges as first-class data types. Not only can you put a
  range of time (or INTs or NUMERICs or whatever) into a single data value, you can use a host of
  built-in operators to manipulate and query ranges safely and quickly. You
  can even apply specially-developed indices to them to massively accelerate
  queries that use these operators.
* PostgreSQL: NUMERIC (and DECIMAL - they're symonyms) is near-as-dammit arbitrary
  precision: it supports 131,072 digits before the decimal point and 16,383 digits after the decimal point.
* PostgreSQL: XML/Xpath querying is supported
* PostgreSQL's logs, by default, are all in one place. By changing a couple of settings in a text file,
  you can get it to log to CSV (and since we're talking about PostgreSQL, it's proper CSV, not broken CSV).
  You can easily set the logging level anywhere from "don't bother logging
  anything" to "full profiling and debugging output".
  The documentation even contains DDL for a table into which the CSV-format
  logs can be conveniently imported.
  You can also log to stderr or the system log or to the Windows event log
  (provided you're running PostgreSQL in Windows, of course).
    The logs themselves are human-readable and machine-readable and contain data
  likely to be of great value to a sysadmin.
    Who logged in and out, at what times, and from where? Which queries are being
  run and by whom?
    How long are they taking? How many queries are submitted in each batch?
  Because the data is well-formatted CSV,
    it is trivially easy to visualise or analyse it in R or PostgreSQL itself or
  Python's matplotlib or whatever you like.
* PostgreSQL comes with a set of extensions called contrib modules. There are libraries of functions,
  types and utilities for doing certain useful things which don't quite fall
  into the core feature set of the server. There are libraries for fuzzy
  string matching, fast integer array handling, external database connectivity,
  cryptography, UUID generation, tree data types and loads, loads more. A few
  of the modules don't even do anything except provide templates to allow
  developers and advanced users to develop their own extensions and custom functionality.


[[{01_PM.resource,data.analytics,02_DOC_HAS.comparative,01_PM.qa]]
# PostgreSQL vs MSQL doc. comparative
@[https://www.linkedin.com/pulse/postgresql-vs-ms-sql-server-girish-chander/]

- MS SQL Server's documentation is all ... unfriendly, sprawling mess,  [humor]
  conservative, humourless, "business appropriate" – i.e. officious,
  boring and dry.
  Not only does it lack amusing references to the historical role of Catholicism
  in the development of date arithmetic, it is impenetrably stuffy and
  hidden behind layers of unnecessary categorisation and ostentatiously
  capitalised official terms. Try this: go to the product documentation
  page for MS SQL Server 2012 and try to get from there to something
  useful. Or try reading this gem (not cherry-picked, I promise):

- A  report part definition is an XML fragment of a report definition
  file. You create report parts by creating a report definition, and
  then selecting report items in the report to publish separately as
  report parts.

- Has the word "report" started to lose its meaning yet?
[[}]]


[[}]]

[[{data.analytics,01_PM.TODO]]
# Regex support
* fundamental in analytics work involving text processing tasks.
  """  A data analytics tool without regex support is like a bicycle without a
      saddle – you can still use it, but it's painful. """
* Great support in PostgreSQL
  WARN: Non-portable to other DDBBs.
* Exs:
  ```
  | > SELECT * FROM my_table
  |      WHERE my_field ~ E'^([0-9])\\1+[aeiou]';   ← Get all lines starting with a repeated digit
  |                                                   followed by a vowel
  |
  | > SELECT SUBSTRING(my_field FROM E'\\y[A-Fa-f0-9]+\\y') ← Get first isolated hex-string
  |   FROM my_table;                                          occurring in a field:
  |
  |
  | > SELECT REGEXP_SPLIT_TO_TABLE('The quick brown fox', E'\\s+'); ← split string based on regex
  |                                                                   return each fragment in a row
  |   │ column │
  |   ├────────┤
  |   │ The    │
  |   │ quick  │
  |   │ brown  │
  |   │ fox    │
  |
  | > SELECT
  |     REGEXP_MATCHES(my_string, E'\\y[a-z]{10,}\\y', 'gi')   ← 'gi' flags:
  |   FROM my_table;                └──────┬───────┘              g: All matches (vs just first match)
  |                                        │                      i: Case insensitive
  |                                        └───────────────────── word with 10+ letters
  |                                 └─────────┬───────────┘
  |                                           └────────────────── find all words (case─insensitive) in my_string
  |                                                               with at least 10 letters:
  ```
[[}]]

[[{data.analytics,01_PM.TODO]]
# Wide Column Extension
- cstore_fdw extension is a columnar store extension for analytics
  use cases where data is loaded in batches.
- Cstore_fdw’s columnar nature delivers performance by only reading relevant
  data from disk.
- It may compress data by 6 to 10 times to reduce space requirements
  for data archive.
[[}]]

[[{data.analytics,01_PM.TODO]]
# TimescaleDB (Time Series) Extension
@[https://www.timescale.com/]
@[https://github.com/timescale/timescaledb]
"Supercharged PostgreSQL"
- Reuse PostgreSQL massive ecosystem.

- 10-100x faster queries than PostgreSQL, InfluxDB, and MongoDB.
  Native optimizations for time-series.

- scale to millions of data points per second per node.
  Horizontally scale to petabytes. Don’t worry about cardinality.

- ask complex questions thanks to Relational+time series integration

- Optionally SaaS on AWS, Azure, or GCP in 75+ regions.

- 94 - 97% compression rates from best-in-class algorithms and
  other performance improvements.

@[https://www.infoq.com/news/2018/10/space-time-series-data]
- European Space Agency Science Data Center
  switched to PostgreSQL with the TimescaleDB extension for their
  data storage.
  ESDC’s diverse data includes structured, unstructured and time
  series metrics running*to hundred of terabytes (per day), and*
 *querying requirements across datasets with open source tools.*
  - Cross referencing of datasets was a requirement while choosing
    a data storage solution, as was the need to be able to use readily
    available, open source tools to analyze the data.
  - PostgreSQL selected for its maturity and support for various data
    types, unstructured data, geo-spatial and time series data,
    native support for JSON and full text search.
  - TS Data analaysis required low write speed requirements but
    queries had to support structured data types, ad-hoc matching
    between datasets and large datasets of up to hundreds of TBs.
    ... It's unclear which alternative time series databases were evaluated,
    but the team did not opt for any of them as they had standardized on
    SQL as the query language of choice,
    - PostgreSQL 10+ partitioning support attempts to solve the problem of
      keeping large table indexes in memory and writing them to disk on every
      update by splitting tables into smaller partitions.
      Partitioning can also be used to store time series data when the
      partitioning is done by time, followed by indices on those partitions.
      ESDC's efforts to store time series data ran into performance issues,
      and they switched to an extension called TimescaleDB.

<!-- @ma -->
* @[https://blog.timescale.com/when-boring-is-awesome-building-a-scalable-time-series-database-on-postgresql-2900ea453ee2]
* TimescaleDB uses an abstraction called a Hypertable
  <https://docs.timescale.com/v1.0/introduction/architecture>
  to hide partitioning across multiple dimensions like time and space.
  Each hypertable is split into "chunks", and each chunk corresponds
  to a specific time interval. Chunks are sized so that all of the B-tree
  structures for a table's indices can reside in memory during inserts,
  similar to how PostgreSQL does partitioning.
  Indices are auto-created on time and the partitioning key.
  Queries can be run against arbitrary dimmensions
  <https://news.ycombinator.com/item?id=14041467> (just like TS ddbb alternatives)
* One  differences between TimescaleDB and other partitioning tools like
  pg_partman is support for auto-sizing of partitions.
* TimescaleDB has reported higher performance benchmarks compared to
  PostgreSQL 10 partitioning based solutions and InfluxDB
  (<https://blog.timescale.com/time-series-data-postgresql-10-vs-timescaledb-816ee808bac5>,
  <https://blog.timescale.com/timescaledb-vs-influxdb-for-time-series-data-timescale-influx-sql-nosql-36489299877>)
* there have been concerns about maintainability
 @[https://news.ycombinator.com/item?id=14041870]
  Clustered deployments of TimescaleDB still under development at the time of writing.
[[}]]

[[{security.autiding,security.monitoring,01_PM.TODO]]
# User Auditing

 *pgaudit and set_user*:
- Some essential auditing features in PostgreSQL are implemented as
  extensions, which can be enabled at will on highly secured
  environments with regulatory requirements.

- pgaudit helps to audit activities like
  - unauthorized user intentionally obfuscated the DDL or DML,
    statement passed and sub-statement actually executed
    will be logged in the PostgreSQL log file.

- set_user  provides a method of privilege escalations. If properly
  implemented, it provides the highest level of auditing, which allows
  the monitoring of even SUPERUSER actions.
[[}]]

[[{architecture.gis,01_PM.TODO]]
# Tools to create graphs in PostGIS:

* osm2pgrouting: (Preinstaled with PostGIS). Creates graphs from OSM cartography.
  Only OpenStreetMap supported (2018).
* pgr_nodeNetwork: (Beta) creates a graphs from table's data.
* Combining PostGIS functions:
  * St_Union() : Allows for "dissolve" operation, joining N geometries in one MULTI
                (MULTILINESTRING,...)
  * St_Dump () : Allows to fetch all geometries in a MULTI one. in a data-composed type
                 formed by 'path' attribute (alphanumeric), and  'geom' attribute (geometry).
* Ex.:
  ```
  | CREATE TABLE  street_guide ( id SERIAL PRIMARY KEY, name VARCHAR(25), geom GEOMETRY );
  |
  | INSERT INTO street_guide (name, geom)
  |   VALUES (‘A’, St_GeomFromText( ‘LINESTRING(
  |                -3.6967738 40.4227273, -3.6966589 40.4231664,
  |                -3.6965874 40.4234147, -3.6965117 40.4236891,
  |                -3.6965125 40.4237212)’
  |           ) );
  |
  | INSERT INTO street_guide (name, geom)
  |   VALUES (‘B’, St_GeomFromText( ‘LINESTRING(
  |                -3.6955342 40.4236784, -3.6955697 40.4231059,
  |                -3.6956075 40.4225342)’
  |           ));
  | ...
  |
  | CREATE TABLE *street_guide_graph* AS
  |   SELECT (ST_DUMP(St_Union(geom))).geom   ← Use ST_DUMP(ST_UNION(...)) to create graph ¹
  |   FROM street_guide
  |
  | ¹ to group ALL geometries we must ignore any other attributes in ST_UNION (aggregate function)
  |   or grouping will fail.
  |   To recover other attributes in each arc and espacial relationship of type ST_CONTAINS() among
  |   streets and arcs must be established.
  |
  | ALTER TABLE street_guide_graph ADD COLUMN source INTEGER;
  | ALTER TABLE street_guide_graph ADD COLUMN target INTEGER;
  |
  |  SELECT pgr_createTopology
  |         (‘street_guide_graph’, 0.00001, ‘geom’,’id’); ← - Create street_guide_graph_vertices_pgr  table
  |                                                          - updates 'source', 'target' in street_guide_graph.
  ```

## PostGIS extension on Mng DBs
* <https://www.scaleway.com/en/docs/using-the-postgis-extension-on-managed-databases/>
[[}]]


[[{ PM.TODO.now ]]

# Expanding PostgreSQL with Extensions 


* <https://www.postgresql.org/docs/current/static/contrib.html>
* <https://www.percona.com/blog/2018/10/05/postgresql-extensions-for-an-enterprise-grade-system/>

  ```
  | => SELECT * FROM pg_available_extensions;     ← check extensions enabled in current ddbb
  |*Ex. Install postgis extension:*
  | $ sudo add-apt-repository ppa:ubuntugis/ppa    ← STEP 1:
  | $ sudo apt-get update
  | $ sudo apt-get install postgis
  |
  | => CREATE EXTENSION postgis;                   ← STEP 2:
  | => CREATE EXTENSION postgis_topology;
  ```
[[PM.TODO.now }]]


[[{architecture,01_PM.TODO]]
# Architecture  [[{devops.monitoring}]]
  ```
  | systemctl status output:
  | /system.slice/postgresql.service CGroup
  |   ├─7117 /usr/bin/postgres -D /var/lib/pgsql/data -p 5432
  |   ├─7118 postgres: logger process
  |   ├─7120 postgres: checkpointer process
  |   ├─7121 postgres: writer process
  |   ├─7122 postgres: wal writer process
  |   ├─7123 postgres: autovacuum launcher process
  |   └─7124 postgres: stats collector process
  |
  | $ ps hf -u postgresq -o cmd
  | /usr/pgsql-9.4/bin/postgres -D /var/lib/pgsql/9.4/data
  | \_ postgres: logger       process
  | \_ postgres: checkpointer process
  | \_ postgres: writer       process
  | \_ postgres: wal writer   process
  | \_ postgres: autovacuum launcher  process
  | \_ postgres: stats collector  process
  | \_ postgres: postgres pgbench [local] idle in transaction
  | \_ postgres: postgres pgbench [local] idle
  | \_ postgres: postgres pgbench [local] UPDATE
  | \_ postgres: postgres pgbench [local] UPDATE waiting
  | \_ postgres: postgres pgbench [local] UPDATE
  ```

# Deep Dive into psql Statistics [[{02_DOC_HAS.diagram}]]
* The Internals of PostgreSQL!!!  @[http://www.interdb.jp/pg/index.html]
  Extracted from <https://dataegret.com/2015/11/deep-dive-into-postgresql-statistics/>
  ```
  |    Client Backends              |                                                  |Postmaster
  |   [pg_stat_activity]            |                                                  |[pg_stat_database]
  | -----------------------         |                                                  |----------------
  |    Query Planning               |  Shared Buffers                                  |Background Workers
  |                                 |  [pg_buffercache]                                |
  | -----------------------         |                                                  |------------------
  | Query Execution                 |                                                  |Autovacuum Launcher
  |                                 |                                                  |
  | =======================         |                                                  |-------------------
  | Indexes IO  | Tables IO         |                                                  |Autovacuum Workers
  |             |                   |                                                  |[pg_stat_activity]
  |             |                   |                                                  |[pg_stat_user_tables]
  | -----------------------         |                                                  |
  |     Buffers IO                  |                                                  |
  | [pg_stat_database]              |                                                  |
  | [pg_statio_all_indexes]         |                                                  |
  | [pg_statio_all_tables]          |                                                  |
  | =============================================================================================================
  |          Write Ahead Log
  |     [pg_current_xlog_location]
  |     [pg_xlog_location_diff]
  | ============================================================================================================
  |                                                   |
  | Logger Process                                    |                        Stats Collector
  |                                                   |
  | ============================================================================================================
  | Logical               | WAL Sender           |Archiver           | Background         |  Checkpointer
  | Replication           |   Process            | Process           |  Writer            |   Process
  | [pr_replication_slots]| [pg_stat_replication]|[pg_stat_archiver] | [pg_stat_bgwriter] |  [pg_stat_database]
  |                       |                      |                   |                    |  [pg_stat_bgwriter]
  | ============================================================================================================
  |                                                  |
  |               NETWORK                            |               STORAGE
  |                                                  |         [pg_stat_kcache]
  |     (nicstat, iostat, ...)                       |         (iostat, ...)
  | ============================================================================================================
  |      WAL Receiver Process     |                           |  Tables/Indexes Data Files
  |                               |                           |  [pg_index_size]     [pgstattupple]
  | ------------------------------|                           |  [pg_table_size]
  |         Recovery Process      |                           |  [pg_database_size]
  |  [pg_stat_database_conflicts] |                           |
  | ==============================+                           +=================================================
  ```
[[}]]

# Jailer "Jail" SQL subset data [[{qa.testing,qa.ux,data.analytics]]
@[https://github.com/Wisser/Jailer]

## Features
* Exports consistent and referentially intact row-sets from
  productive database and imports into dev/test environment.
* Improves database performance by removing and archiving obsolete   [[{performance,architecture.scalability]]
  data without violating integrity.                                  [[}]]
* Generates topologically sorted SQL-DML, hierarchically structured
  XML and DbUnit datasets.
* Data Browsing. Navigate bidirectionally through the database by
  following foreign-key-based or user-defined relationships.
* SQL Console with code completion, syntax highlighting and
  database metadata visualization.
[[}]]

[[{01_PM.TODO]]
https://www.enterprisedb.com/blog/postgresql-wal-write-ahead-logging-management-strategy-tradeoffs

## "ORDER BY" improved our query times 100x [[{performance.101]]
* <https://webapp.io/blog/postgres-query-speedups/>
* KeyPoint:<br/>
  queries can be sped up by creating right indices,
  and making sure that they are used.

  ```
  | Context:
  |   table
  |   ci_jobs  <·· inserts whenever a developer commits
  |   -------      tracks CI pipeline status for that commit.
  |                VITALLY IMPORTANT: status updates are pushed to XXX on time.
  | 
  |   status_change_time timestamp \          <·· when status has been changed
  |                      NOT NULL DEFAULT NOW(),
  |   status_notify_time timestamp \          <·· when status has been notified to XXX
  |                      NOT NULL DEFAULT to_timestamp(0));
  | 
  | |listener| Component:  polls ci_job rows which have been changed,
  |                        but for which XXX hasn't been recently told about
  | 
  | UPDATE ci_jobs
  |   SET status_notify_time = status_change_time
  |   WHERE id IN (
  |     SELECT ci_jobs.id FROM ci_jobs
  |       WHERE ci_jobs.status_notify_time != ci_jobs.status_change_time
  |         AND ci_jobs.status_notify_time < NOW()-'10 seconds'::interval LIMIT 10
  |         FOR UPDATE SKIP LOCKED
  |     )
  |     RETURNING idAutomatic warnings for slow queries
  | 
  | => EXPLAIN ANALYZE
  |     SELECT ci_jobs.id FROM ci_jobs
  |       WHERE ci_jobs.status_notify_time != ci_jobs.status_change_time
  |         AND ci_jobs.status_notify_time < NOW()-'10 seconds'::interval LIMIT 10
  |         FOR UPDATE SKIP LOCKED;
  | 
  |    Paste the result to explain.dalibo.com to see a GUI friendly output.
  |    It was simple enough to create a SQL Index that would help PostgreSQL
  |    find such rows faster:
  ```
* First FIX Attemp:
  ```
  | CREATE INDEX to_be_updated_ci_jobs
  |   ON ci_jobs(id, status_notify_time ASC)
  |   WHERE (ci_jobs.status_notify_time != ci_jobs.status_change_time);
  |   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  |   An index can be created CONDITIONALLY for rows not matching a criteria [[keypoint]]
  |   The index is basically an ordered set of the rows which have
  |   different status_notify_time and status_change_time fields,
  |   ordered by status_notify_time ascending.
  |
  | FAIL: the index wasn't being used according to EXPLAIN ANALYZE.
  |
  | FAILURE HYPOTHESIS: we are choosing 10 arbitrary rows (vs10 oldest)
  |
  | By indicating that we want the 10 oldest rows the index is used!!!:
  ```

  ```
  |  EXPLAIN ANALYZE
  |     SELECT ci_jobs.id FROM ci_jobs
  |       WHERE ci_jobs.status_notify_time != ci_jobs.status_change_time
  |         AND ci_jobs.status_notify_time < NOW()-'10 seconds'::interval
  | +       ORDER BY ci_jobs.status_notify_time
  | +       LIMIT 10
  |         FOR UPDATE SKIP LOCKED;
  | Execution time plummeted 2000ms ···> 24ms!!!!.
  ```
* NOTE: the query estimator might better understand why this index is so
  small (very few jobs are being changed, most are for commits long
  passed) and we'll be able to drop the ORDER BY, but for now it's
  required for a 100x speed up.

[[performance.101}]]


